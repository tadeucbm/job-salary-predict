{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b7077ce",
   "metadata": {},
   "source": [
    "# 0.0. Planejamento\n",
    "##  Input\n",
    "### Dados: \n",
    "* **Id**: Identificador Único;\n",
    "* **Title**: Título da vaga;\n",
    "* **FullDescription**: Texto completo com a descrição da vaga;\n",
    "* **LocationRaw**: Texto cru da localizacao da vaga;\n",
    "* **LocationNormalized**: Localizacao normalizada - **Carrega erro**;\n",
    "* **ContractType**: Tipo de contrato. Indica como é o regime de horas da vaga. Pode ser:\n",
    "    * full_time;\n",
    "    * part_time.\n",
    "* **ContractTime**: Tempo de validade do contrato. Pode ser:\n",
    "    * permanent;\n",
    "    * contract.\n",
    "* **Company**: Nome da compania que está contratanto;\n",
    "* **Category**: 30 categorias de trabalho - **Carrega erro**;\n",
    "* **SalaryNormalised**: Salario anualizado da vaga. O que estamos buscando prever;\n",
    "* **SourceName**: Nome to site o qual recebemos a vaga.\n",
    "\n",
    "### Problema:\n",
    "* Temos um conjunto de dados com as informacoes de vagas abertas. Com base nessas \\\n",
    "informacoes, devemos criar um algoritmo capaz de prever o salário em novas vagas.\n",
    "\n",
    "## Output\n",
    "* **Modelo**: Um modelo capaz de prever o salario de novos usuarios;\n",
    "* **WebApp**: Um webapp que carregue esse modelo e faça previsões;\n",
    "* **EDA**: Uma Análise Exploratória dos Dados e do Modelo:\n",
    "    - Relatório do Modelo;\n",
    "    - Relatório do Projeto.\n",
    "\n",
    "\n",
    "## Tasks\n",
    "1. Planejamento;\n",
    "2. Ciclos;\n",
    "3. Construcao do Modelo Final;\n",
    "    - Métrica Principal: Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118c4f49",
   "metadata": {},
   "source": [
    "# 1.0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a12ea19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tadeu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/tadeu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tadeu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# api\n",
    "import requests\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Plot\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from PIL import Image\n",
    "\n",
    "# Model\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# stats\n",
    "from scipy.stats import pointbiserialr, pearsonr\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07443869",
   "metadata": {},
   "source": [
    "## 1.1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3124a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jupyter_settings():\n",
    "    \"\"\"\n",
    "    Retorna algumas configuracoes para o jupyter notebook\n",
    "    \"\"\"\n",
    "    %matplotlib inline\n",
    "    %pylab inline\n",
    "    \n",
    "    plt.style.use('ggplot')\n",
    "    plt.rcParams['figure.figsize'] = [24, 9]\n",
    "    plt.rcParams['font.size'] = 24\n",
    "    \n",
    "    display(HTML('<style>.container{width:100% !important;}</style>'))\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.options.display.max_rows = None\n",
    "    pd.set_option('display.expand_frame_repr', False)\n",
    "    \n",
    "    sns.set()\n",
    "    \n",
    "\n",
    "\n",
    "def cross_val_performance(X_train, y_train, model, cv):\n",
    "    \"\"\"\n",
    "    Retorna algumas o RMSE, MAE e MAPE em cross validation do modelo\n",
    "    Input:\n",
    "        X_train: os dados de treino\n",
    "        y_train: A variavel resposta\n",
    "        model: modelo a ser usado\n",
    "        cv: numero de folds\n",
    "    Output:\n",
    "        Retorna um dataframe com as metricas de RMSE, MAE, MAPE e o tempo\n",
    "        médio de processamento.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Cross Val Train\n",
    "    results = cross_validate(model, \n",
    "                             X_train, \n",
    "                             y_train, \n",
    "                             cv=cv,\n",
    "                             scoring=[\n",
    "                                 'neg_mean_absolute_error', \n",
    "                                 'neg_mean_absolute_percentage_error', \n",
    "                                 'neg_root_mean_squared_error'\n",
    "                             ]\n",
    "                            )\n",
    "    \n",
    "    # Performance\n",
    "    mae = round(np.mean(results['test_neg_mean_absolute_error'] * -1), 2)\n",
    "    mape = round(np.mean(results['test_neg_mean_absolute_percentage_error'] * -1), 4)\n",
    "    rmse = round(np.mean(results['test_neg_root_mean_squared_error'] * -1), 2)\n",
    "    time = round(np.mean(results['fit_time']), 2)\n",
    "    \n",
    "    df_results = pd.DataFrame(\n",
    "                {\n",
    "                    'Modelo': type(model).__name__,\n",
    "                    'MAE': mae,\n",
    "                    'MAPE': mape,\n",
    "                    'RMSE': rmse,\n",
    "                    'time': time\n",
    "                },\n",
    "                index=[0]\n",
    "            )\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "\n",
    "def baseline_performance(y_train, n_splits):\n",
    "    \"\"\"\n",
    "    Return a cross validation metric from the baseline model - mean\n",
    "    Input:\n",
    "        y_train: array with train response variables\n",
    "        n_splits: number of folds\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    \n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    rmse_list = []\n",
    "    \n",
    "    for train_index, valid_index in kf.split(y_train): # split index train - valid\n",
    "        \n",
    "        new_y_train = y_train.iloc[train_index]\n",
    "        new_y_valid = y_train.iloc[valid_index]\n",
    "        \n",
    "        y_hat = np.repeat(new_y_train.mean(), len(new_y_valid))\n",
    "        \n",
    "        mae_list.append(mean_absolute_error(new_y_valid, y_hat))\n",
    "        mape_list.append(mean_absolute_percentage_error(new_y_valid, y_hat))\n",
    "        rmse_list.append(sqrt(mean_squared_error(new_y_valid, y_hat)))\n",
    "        \n",
    "    df_results = pd.DataFrame(\n",
    "                {\n",
    "                    'Modelo': 'Baseline',\n",
    "                    'MAE': round(np.mean(mae_list), 3),\n",
    "                    'MAPE': round(np.mean(mape_list), 3),\n",
    "                    'RMSE': round(np.mean(rmse_list), 3),\n",
    "                    'time': 0\n",
    "                },\n",
    "                index=[0]\n",
    "            )\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "\n",
    "def tokenize(text, perm=False):\n",
    "    # normalize case and remove punctuation\n",
    "    if perm:\n",
    "        text = re.sub(r\"[^a-zA-Z0-9-]\", \" \", text.lower())\n",
    "    else:\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize andremove stop words and remove single letters:\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if (word not in stop_words) and (len(word) >= 2)]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def comparison_continuos(df=[], names=[]):\n",
    "    \n",
    "    for i, dataframe in enumerate(df):\n",
    "        n_df = dataframe.describe()\n",
    "        n_df.loc['skew'] = dataframe.skew()\n",
    "        n_df.loc['kurtosis'] = dataframe.kurtosis()\n",
    "        \n",
    "        n_df.name = names[i]\n",
    "        \n",
    "        if i == 0:\n",
    "            final_df = n_df\n",
    "        else:\n",
    "            final_df = pd.concat([final_df, n_df], axis=1)\n",
    "        \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def city_imputation(df, column_city_raw, column_city_normalized, list_city):\n",
    "    \"\"\"\n",
    "    Retorna uma series com as cidades extraidas da api e seu index de referencia\n",
    "    Input:\n",
    "        df - Dataframe usado\n",
    "        column_city_raw - Coluna raw de cidades\n",
    "        column_city_normalized - Coluna normaliada de cidades\n",
    "        list_city = lista de cidades da api\n",
    "    Output:\n",
    "        Series com cada index associado a sua cidade extraida da referencia da api\n",
    "        ou do coalesce com a coluna normalizada original\n",
    "    \"\"\"    \n",
    "    \n",
    "    df_aux = df.loc[:, [column_city_raw, column_city_normalized]]\n",
    "    \n",
    "    # For loop por cada cidade na lista da api\n",
    "    for city in list_city:\n",
    "\n",
    "        df_aux['aux'] = df_aux['LocationRaw'].apply(lambda x: city if city.lower() in x.lower() else None)\n",
    "        index_val = df_aux.loc[df_aux['aux'] == city, :].index.values\n",
    "\n",
    "        for num_index in index_val:\n",
    "            dict_values[num_index] = city\n",
    "            \n",
    "    # Criando um DataFrame com as cidades e index\n",
    "    new_city = pd.DataFrame(dict_values, index=['City']).T\n",
    "\n",
    "    # Merge com as cidades originais\n",
    "    merge_city = pd.merge(df_aux, new_city, left_index=True, right_index=True, how='left').loc[:, ['LocationNormalized', 'City']]\n",
    "\n",
    "    # Coalesce ente as colunas\n",
    "    city_series = merge_city['City'].combine_first(merge_city['LocationNormalized'])\n",
    "\n",
    "    return city_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9245d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformText():\n",
    "    \"\"\"\n",
    "    Classe usada para transformar uma coluna de textos em dummies de palavras\n",
    "    e extrair informacoes relevantes como:\n",
    "        1. A frequencia de cada palavra;\n",
    "        2. A correlacao da palavra com a variavel resposta;\n",
    "        3. O p_value dessa correlacao.\n",
    "    \n",
    "    Attributes:\n",
    "        dataframe (dataframe pandas) - dataframe a ser utilizado para treino;\n",
    "        text_col (str) - nome da coluna contendo texto;\n",
    "        response_col (str) - nome da coluna com a variavle resposta continua.\n",
    "        \n",
    "    OBS:\n",
    "        Por ser um protótipo, essa classe segue alguns preceitos desse projeto \n",
    "        especifico:\n",
    "            1. Necessita de uma variavel resposta;\n",
    "            2. Essa variavel resposta tem que ser continua.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, text_col='', response_col=''):\n",
    "        \n",
    "        self.text_name = text_col\n",
    "        self.response_name = response_col\n",
    "        self.dataframe = df.loc[:, [self.text_name, self.response_name]]\n",
    "    \n",
    "\n",
    "    def unique_tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Funcao utilizada para aplicar o processo de tokenizacao de um texto\n",
    "        Input:\n",
    "            text: Texto que sera tokenizado\n",
    "        Output:\n",
    "            uma lista de palavras que passam pelas seguintes etapas:\n",
    "                1. Sao selecionados apenas valores a-zA-Z0-9;\n",
    "                2. Todo é colocado em minusculo;\n",
    "                3. O texto é tokenizado com word_tokenize;\n",
    "                4. o texto é lemmatizado e as stop_words sao retiradas;\n",
    "                5. Apenas valores com tamanho maior que 2 sao selecionados.\n",
    "        \"\"\"\n",
    "        \n",
    "        # normalize case and remove punctuation\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "\n",
    "        # tokenize text\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # lemmatize andremove stop words and remove single letters:\n",
    "        tokens = [\n",
    "            lemmatizer.lemmatize(word) \n",
    "            for word \n",
    "            in tokens \n",
    "            if (word not in stop_words) \n",
    "            and (len(word) >= 2)\n",
    "        ]\n",
    "        \n",
    "        return tokens\n",
    "        \n",
    "        \n",
    "    def tokenize(self):\n",
    "        \"\"\"\n",
    "        Funcao utilizada para aplicar a funcao unique_tokenize em todas as \n",
    "        linhas de text_name e guardar em self.tokens\n",
    "        \"\"\"\n",
    "        self.tokens = self.dataframe[self.text_name].apply(lambda x: self.unique_tokenize(x))\n",
    "        \n",
    "        return self.tokens\n",
    "    \n",
    "    \n",
    "    def get_frequency(self):\n",
    "        \"\"\"\n",
    "        Retorna a frequencia absoluta de cada palavra em text_name\n",
    "        \"\"\"\n",
    "        # gerando o dataframe com as contagens\n",
    "        final_df = self.tokenize().explode().value_counts().reset_index().rename({'index': 'Word',\n",
    "                                                                                  self.text_name: 'Count'},\n",
    "                                                                                 axis=1)\n",
    "        \n",
    "        self.frequency_ = final_df.sort_values(by='Count', ascending=False)\n",
    "        \n",
    "        return self.frequency_\n",
    "\n",
    "    \n",
    "    def unique_words(self, sample_size):\n",
    "        \"\"\"\n",
    "        Gera uma lsita das palavras unicas de acordo com o tamanho desejado\n",
    "        Input:\n",
    "            sample_size (int64) - quantas linhas quero retornar.\n",
    "        Output:\n",
    "            Retorna uma lista com os valores unicos mais frequentes na ordem\n",
    "            de grandeza de sample_size.\n",
    "        \"\"\"        \n",
    "        \n",
    "        return self.get_frequency().head(sample_size)['Word'].unique()\n",
    "        \n",
    "    \n",
    "\n",
    "    def get_corr(self, sample_size):\n",
    "        \"\"\"\n",
    "        Faz o calculo de correlacao utilizando o metodo pointbiserial\n",
    "        Input:\n",
    "            sample_size (int64) - sample_size para contar o numero de palavras\n",
    "            desejadas em unique_words\n",
    "        Output:\n",
    "            Um DataFrame com as seguintes colunas:\n",
    "                Word - A palavra utilizada;\n",
    "                Corr - A correlacao entre a palavra e a variável resposta;\n",
    "                p-value - o p-value dessa correlacao.\n",
    "        \"\"\"\n",
    "        dict_words = {} # dicionario a ser alimentado\n",
    "        \n",
    "        list_words = self.unique_words(sample_size) # chamando a funcao com limite de palavras\n",
    "        array_x = self.tokens # determinadno o array com os tokens\n",
    "        array_y = self.dataframe[self.response_name].values # determinando o array com a var resposta\n",
    "        \n",
    "        for word in list_words: # iterando por cada palavra selecionada\n",
    "            \n",
    "            # se a palavra estiver no texto, atribui-se 1, senao atribui-se 0\n",
    "            list_values = [1 if word in row else 0 for row in array_x] \n",
    "            # calculando a correlacao através do método do ponto bisserial\n",
    "            result = pointbiserialr(list_values, self.dataframe[self.response_name].values)\n",
    "            \n",
    "            dict_words[word] = [result[0], result[1]]\n",
    "            \n",
    "        self.corr_ = pd.DataFrame(dict_words, index=['Corr', 'p-value']).T.sort_values(by='Corr')\n",
    "\n",
    "        return self.corr_\n",
    "\n",
    "\n",
    "    def transform(self, sample_size, threshold=0.0):\n",
    "        \"\"\"\n",
    "        gera um DataFrame com cada palavra sendo uma coluna sendo que:\n",
    "            1 -> a palavra esta no texto;\n",
    "            2 -> a palavra nao esta no texto.\n",
    "        Input:\n",
    "            sample_size (int64) - o numero de palavras em unique_words;\n",
    "            theshold (float64) - a correlacao minima de uma palavra para ser considerada.\n",
    "        \"\"\"\n",
    "\n",
    "        # transformando as correlacoes em valores absolutos\n",
    "        df_cor = self.get_corr(sample_size).apply(lambda x: abs(x))\n",
    "\n",
    "        # Colunas que serao usadas\n",
    "        selected_cols = df_cor.loc[df_cor['Corr'] > threshold, :].index.to_list()\n",
    "        \n",
    "        self.list_words = selected_cols\n",
    "        \n",
    "        df_text = pd.DataFrame(self.tokens)\n",
    "        \n",
    "        # imputando as colunas\n",
    "        for col in selected_cols:\n",
    "            coluna = pd.Series(df_text[self.text_name].apply(lambda x: 1 if col in x else 0,), name=col,index=df_text.index.to_list())\n",
    "            df_text = pd.concat([df_text, coluna], axis=1)\n",
    "            \n",
    "        self.dummy_cols_ = df_text.drop([self.text_name], axis=1)\n",
    "        \n",
    "    \n",
    "    def get_all(self):\n",
    "        \"\"\"\n",
    "        Retorna:\n",
    "            df_corr_freq: dataframe com a frequencia, correlacao e p_value de cada palavra;\n",
    "            dummy_cols: dataframe com as colunas dummy.\n",
    "        \"\"\"\n",
    "        \n",
    "        df_corr_freq = pd.merge(self.frequency_, self.corr_, left_on='Word', right_index=True)\n",
    "        \n",
    "        return df_corr_freq, self.dummy_cols_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e56e3bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container{width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jupyter_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde987ea",
   "metadata": {},
   "source": [
    "## 1.2. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "54afe267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12612628</td>\n",
       "      <td>Engineering Systems Analyst</td>\n",
       "      <td>Engineering Systems Analyst Dorking Surrey Sal...</td>\n",
       "      <td>Dorking, Surrey, Surrey</td>\n",
       "      <td>Dorking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12612830</td>\n",
       "      <td>Stress Engineer Glasgow</td>\n",
       "      <td>Stress Engineer Glasgow Salary **** to **** We...</td>\n",
       "      <td>Glasgow, Scotland, Scotland</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 35000/annum 25-35K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12612844</td>\n",
       "      <td>Modelling and simulation analyst</td>\n",
       "      <td>Mathematical Modeller / Simulation Analyst / O...</td>\n",
       "      <td>Hampshire, South East, South East</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 40000/annum 20-40K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12613049</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 30000/annum 25K-30K negotiable</td>\n",
       "      <td>27500</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12613647</td>\n",
       "      <td>Pioneer, Miser Engineering Systems Analyst</td>\n",
       "      <td>Pioneer, Miser  Engineering Systems Analyst Do...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              Title                                    FullDescription                        LocationRaw LocationNormalized ContractType ContractTime                       Company          Category                               SalaryRaw  SalaryNormalized        SourceName\n",
       "0  12612628                        Engineering Systems Analyst  Engineering Systems Analyst Dorking Surrey Sal...            Dorking, Surrey, Surrey            Dorking          NaN    permanent  Gregory Martin International  Engineering Jobs              20000 - 30000/annum 20-30K             25000  cv-library.co.uk\n",
       "1  12612830                            Stress Engineer Glasgow  Stress Engineer Glasgow Salary **** to **** We...        Glasgow, Scotland, Scotland            Glasgow          NaN    permanent  Gregory Martin International  Engineering Jobs              25000 - 35000/annum 25-35K             30000  cv-library.co.uk\n",
       "2  12612844                   Modelling and simulation analyst  Mathematical Modeller / Simulation Analyst / O...  Hampshire, South East, South East          Hampshire          NaN    permanent  Gregory Martin International  Engineering Jobs              20000 - 40000/annum 20-40K             30000  cv-library.co.uk\n",
       "3  12613049  Engineering Systems Analyst / Mathematical Mod...  Engineering Systems Analyst / Mathematical Mod...     Surrey, South East, South East             Surrey          NaN    permanent  Gregory Martin International  Engineering Jobs  25000 - 30000/annum 25K-30K negotiable             27500  cv-library.co.uk\n",
       "4  12613647         Pioneer, Miser Engineering Systems Analyst  Pioneer, Miser  Engineering Systems Analyst Do...     Surrey, South East, South East             Surrey          NaN    permanent  Gregory Martin International  Engineering Jobs              20000 - 30000/annum 20-30K             25000  cv-library.co.uk"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_import = pd.read_csv('../data/Train_rev1.csv')\n",
    "\n",
    "data_import.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a566157",
   "metadata": {},
   "source": [
    "# 2.0. Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd3b7a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = data_import.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9324cb",
   "metadata": {},
   "source": [
    "## 2.1. Shape Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "08328053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 244768\n",
      "Columns: 12\n"
     ]
    }
   ],
   "source": [
    "print(f'Rows: {df2.shape[0]}')\n",
    "print(f'Columns: {df2.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172c5cf",
   "metadata": {},
   "source": [
    "## 2.2. Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c254ee69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                     int64\n",
       "Title                 object\n",
       "FullDescription       object\n",
       "LocationRaw           object\n",
       "LocationNormalized    object\n",
       "ContractType          object\n",
       "ContractTime          object\n",
       "Company               object\n",
       "Category              object\n",
       "SalaryRaw             object\n",
       "SalaryNormalized       int64\n",
       "SourceName            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57172a1",
   "metadata": {},
   "source": [
    "## 2.3. Check Na\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e35dea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percentual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FullDescription</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LocationRaw</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LocationNormalized</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ContractType</th>\n",
       "      <td>179326.0</td>\n",
       "      <td>73.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ContractTime</th>\n",
       "      <td>63905.0</td>\n",
       "      <td>26.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company</th>\n",
       "      <td>32430.0</td>\n",
       "      <td>13.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SalaryRaw</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SourceName</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Total  Percentual\n",
       "Id                       0.0        0.00\n",
       "Title                    1.0        0.00\n",
       "FullDescription          0.0        0.00\n",
       "LocationRaw              0.0        0.00\n",
       "LocationNormalized       0.0        0.00\n",
       "ContractType        179326.0       73.26\n",
       "ContractTime         63905.0       26.11\n",
       "Company              32430.0       13.25\n",
       "Category                 0.0        0.00\n",
       "SalaryRaw                0.0        0.00\n",
       "SalaryNormalized         0.0        0.00\n",
       "SourceName               1.0        0.00"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_na = df2.isnull().sum()\n",
    "percentage_na = round(df2.isnull().mean(), 4) * 100\n",
    "\n",
    "# dataframe of na\n",
    "pd.DataFrame([tot_na, percentage_na], index=['Total', 'Percentual']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8161c1",
   "metadata": {},
   "source": [
    "# 3.0. Variable Filtering + Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "49bec6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3f16d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop(['Id', 'SalaryRaw', 'SourceName', 'Company'], axis=1, inplace=True)\n",
    "df3.dropna(subset=['Title'], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d8283",
   "metadata": {},
   "source": [
    "## 3.1. Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9c7de197",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df3.loc[:, 'SalaryNormalized']\n",
    "X = df3.drop('SalaryNormalized', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f6ee7e",
   "metadata": {},
   "source": [
    "# 4.0. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c170c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train merge with y\n",
    "X_train = pd.merge(X_train, y_train, right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dd6b5613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list de stop_words\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# instanciando o lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e3d2e6",
   "metadata": {},
   "source": [
    "## 4.1. Title - Salvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac440b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando a classe TransformText()\n",
    "transf_title = TransformText(df=X_train, text_col='Title', response_col='SalaryNormalized')\n",
    "\n",
    "# transformando as colunas\n",
    "transf_title.transform(sample_size=2500, threshold=0.005)\n",
    "\n",
    "# recebendo as informacoes: Count, Corr e p-value | colunas dummys\n",
    "df_infos_title, dummy_cols_title = transf_title.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0da7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df's in csv file\n",
    "dummy_cols_title.to_csv('../data/dummy_cols_title_3.csv')\n",
    "#df_infos_title.to_csv('../data/df_infos_title.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a87dab",
   "metadata": {},
   "source": [
    "## 4.2. Full Description - Salvo & Retirado C03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d0dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## instanciando a classe TransformText()\n",
    "#transf_desc = TransformText(df=X_train, text_col='FullDescription', response_col='SalaryNormalized')\n",
    "#\n",
    "## transformando as colunas\n",
    "#transf_desc.transform(sample_size=2000, threshold=0.02)\n",
    "#\n",
    "## recebendo as informacoes: Count, Corr e p-value | colunas dummys\n",
    "#df_infos_desc, dummy_cols_desc = transf_desc.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481696dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save df's in csv file\n",
    "#df_infos_desc.to_csv('../data/df_infos_desc.csv')\n",
    "#dummy_cols_desc.to_csv('../data/dummy_cols_desc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc0fc6",
   "metadata": {},
   "source": [
    "## 4.3. SourceName - Retirado C04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a3c052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = X_train['SourceName']\n",
    "#\n",
    "#array_y = y_train.values\n",
    "#\n",
    "#dict_results = {}\n",
    "#\n",
    "#for column in source_dummies.columns:\n",
    "#    array_x = source_dummies[column].values\n",
    "#    \n",
    "#    result = pointbiserialr(array_x, array_y)\n",
    "#    \n",
    "#    dict_results[column] = [result[0], result[1]]\n",
    "#    \n",
    "#source_dummies = pd.get_dummies(a)\n",
    "#\n",
    "#df = pd.DataFrame(dict_results, index=['Corr', 'p-value']).T\n",
    "#\n",
    "#df['Corr_abs'] = df['Corr'].apply(lambda x: abs(x))\n",
    "#\n",
    "#df_sorted = df.sort_values('Corr_abs', ascending=False)\n",
    "#\n",
    "#dummies_source = source_dummies.loc[:, df_sorted[df_sorted['Corr_abs'] >= 0.15].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef2c75",
   "metadata": {},
   "source": [
    "## 4.4. Company - Retirado C04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dum = pd.get_dummies(X_train['Company'])\n",
    "#\n",
    "#array_y = y_train.values\n",
    "#\n",
    "#dict_results = {}\n",
    "#\n",
    "#for column in dum.columns:\n",
    "#    array_x = dum[column].values\n",
    "#    \n",
    "#    result = pointbiserialr(array_x, array_y)\n",
    "#    \n",
    "#    dict_results[column] = [result[0], result[1]]\n",
    "#    \n",
    "#df = pd.DataFrame(dict_results, index=['Corr', 'p-value']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaafb5b",
   "metadata": {},
   "source": [
    "## 4.5. Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8116950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando um DataFrame auxiliar\n",
    "dummies_city = X_train.loc[:, ['LocationRaw', 'LocationNormalized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e2b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo uma requisicao na api para pegar as cidades de UK\n",
    "\n",
    "url = 'https://countriesnow.space/api/v0.1/countries/cities'\n",
    "myobj = {\n",
    "    \"country\": \"United Kingdom\"\n",
    "}\n",
    "\n",
    "x = requests.post(url, data = myobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b80452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista das cidades da API\n",
    "city_api_list = pd.Series(x.json()['data']).to_list()\n",
    "\n",
    "# Criando um Dicionario que associa o index com cada cidade\n",
    "dict_values = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_new_city = city_imputation(X_train, 'LocationRaw', 'LocationNormalized', city_api_list)\n",
    "#\n",
    "## Salvando a series\n",
    "#column_new_city.to_csv('../data/new_location.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60b1233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o csv\n",
    "columns_city = pd.read_csv('../data/new_location.csv', index_col=0)\n",
    "\n",
    "# Merge com os dados originais\n",
    "X_train = pd.merge(X_train, columns_city, left_index=True, right_index=True)\n",
    "\n",
    "# Excluindo a coluna original\n",
    "X_train = X_train.drop('LocationNormalized', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atribuindo nomes a Londres\n",
    "X_train['City'] = X_train['City'].apply(lambda x: 'London' if 'London' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944075d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste de correlacao\n",
    "dum = pd.get_dummies(X_train['City'])\n",
    "\n",
    "array_y = y_train.values\n",
    "\n",
    "dict_results = {}\n",
    "\n",
    "for column in dum.columns:\n",
    "    array_x = dum[column].values\n",
    "    \n",
    "    result = pointbiserialr(array_x, array_y)\n",
    "    \n",
    "    dict_results[column] = [result[0], result[1]]\n",
    "    \n",
    "df = pd.DataFrame(dict_results, index=['Corr', 'p-value']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cd5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.sort_values(by='Corr', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216bd94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_corr_city = df_corr.head(5).index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c05ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = X_train.loc[:, ['City', 'LocationRaw']]\n",
    "\n",
    "for column in list_corr_city:\n",
    "    df_aux[column] = df_aux['City'].apply(lambda x: 1 if column == x else 0)\n",
    "\n",
    "dummies_city = df_aux.drop(['City', 'LocationRaw'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c395a",
   "metadata": {},
   "source": [
    "# 5.0. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a779fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_blue = '#5975a4'\n",
    "color_red = '#CD5C5C'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578bbd49",
   "metadata": {},
   "source": [
    "## 5.1. Response Variable - SalaryNormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e1e465",
   "metadata": {},
   "source": [
    "### 5.1.1. Distribuicao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32be1af",
   "metadata": {},
   "source": [
    "A Variável Resposta é **Distribuicao Normal Assimétrica a Direita**\n",
    "- Média > Mediana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c932cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_continuos([y_train], ['Normal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "# plot histogram\n",
    "sns.histplot(y_train, ax=ax[0], bins=100)\n",
    "\n",
    "# plot boxplot\n",
    "sns.boxplot(x=y_train, ax=ax[1])\n",
    "\n",
    "# title\n",
    "ax[0].title.set_text('Distribuicao Da Variável Salário')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2680e4",
   "metadata": {},
   "source": [
    "### 5.1.2. Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3bb3d9",
   "metadata": {},
   "source": [
    "**Método do Intervalo Interquartil para a idenficicacao de Outliers:**   \n",
    "\n",
    "IQR = Q3 - Q1  \n",
    "\n",
    "High Outlier = Q3 + (1.5 * IQR)  \n",
    "\n",
    "**Limite Superior**: 74.000\n",
    "\n",
    "**Dois testes a serem Feitos em Pŕoximo Ciclo:**  \n",
    "    * Retirar valores acima de 100k  \n",
    "    * Retirar valores acima de 74k  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed59152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo de outliers superiores - Nossos dados nao possuem outliers inferiores\n",
    "high_outlier = y_train.quantile([0.75]) + (y_train.quantile([0.75]).values - y_train.quantile([0.25]).values) * 1.5\n",
    "\n",
    "print(f'Limite Superior - Outliers: {high_outlier.values[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae449b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_outliers = y_train[y_train > 74000]\n",
    "\n",
    "print(f'% de Entradas maiores que 74.000: {round((len(y_outliers) / len(y_train) * 100), 2)}%')\n",
    "print(f'% de Entradas maiores que 100.000: {round((len(y_outliers[y_outliers>100000]) / len(y_train) * 100), 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5703f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_outliers.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675effb4",
   "metadata": {},
   "source": [
    "## 5.2. ContractType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d44273a",
   "metadata": {},
   "source": [
    "**Null Values**:\n",
    "* Os valores de NA possuem uma distribuicao bem semelhante ao Contract_Full:\n",
    "    * Podemos ajustar a premissa de que se na descricao nao se especifica   \n",
    "    o tipo de contrato, está implítico que é full_time.\n",
    "    * Realizar um teste de hipotese para confirmar isso no próximo ciclo.\n",
    "    \n",
    "**Distribuicao**:\n",
    "* Como é de se esperar, contratos de meio período indicam um menor salario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d235ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data\n",
    "contract_null = df2.loc[df2['ContractType'].isnull(), 'SalaryNormalized']\n",
    "contract_full = df2.loc[df2['ContractType'] == 'full_time', 'SalaryNormalized']            \n",
    "contract_part = df2.loc[df2['ContractType'] == 'part_time', 'SalaryNormalized']      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b734b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_continuos([y_train, contract_null, contract_full, contract_part], \n",
    "                     ['Normal', 'Contract_null', 'Contract_full', 'Contract_part'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "sns.countplot(y=X_train['ContractType'].apply(lambda x: str(x)), \n",
    "              order=['nan', 'full_time', 'part_time'], \n",
    "              color=color_blue)\n",
    "\n",
    "# title\n",
    "plt.title('Contagem de Valores ContractType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9146a4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1)\n",
    "\n",
    "# plots\n",
    "sns.boxplot(x=contract_null, ax=ax[0])\n",
    "sns.boxplot(x=contract_full, ax=ax[1])\n",
    "sns.boxplot(x=contract_part, ax=ax[2])\n",
    "\n",
    "# Titles\n",
    "ax[0].title.set_text('Null Values')\n",
    "ax[1].title.set_text('full_time')\n",
    "ax[2].title.set_text('part_time')\n",
    "\n",
    "# Colocando os eixos em mesma ordem\n",
    "for value in range(0, 3):\n",
    "    ax[value].set_xlim([0, 210000])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5098fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "# plots\n",
    "sns.histplot(x=contract_null, ax=ax[0], bins=100)\n",
    "sns.histplot(x=contract_full, ax=ax[1], bins=100)\n",
    "\n",
    "# title\n",
    "ax[0].title.set_text('Null Values Salary')\n",
    "ax[1].title.set_text('Full Time Salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c0c24",
   "metadata": {},
   "source": [
    "## 5.3. ContractTime\n",
    "**Null Values**:\n",
    "* Um quantidade consideravel de Null Values;\n",
    "\n",
    "**Distribuicao**:\n",
    "* A distribuicao dos 3 tipos sao bem semelhantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f55b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data\n",
    "null = df2.loc[df2['ContractTime'].isnull(), 'SalaryNormalized']\n",
    "contract = df2.loc[df2['ContractTime'] == 'contract', 'SalaryNormalized']            \n",
    "permanent =  df2.loc[df2['ContractTime'] == 'permanent', 'SalaryNormalized']      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c622de",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_continuos([y_train, null, contract, contract], \n",
    "                     ['Normal', 'Null', 'Contract', 'Contract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "sns.countplot(y=X_train['ContractTime'].apply(lambda x: str(x)), \n",
    "              order=['permanent', 'nan', 'contract'], \n",
    "              color=color_blue)\n",
    "\n",
    "# title\n",
    "plt.title('Contagem de Valores ContractType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb1e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1)\n",
    "\n",
    "# plots\n",
    "sns.boxplot(x=null, ax=ax[0])\n",
    "sns.boxplot(x=contract, ax=ax[1])\n",
    "sns.boxplot(x=permanent, ax=ax[2])\n",
    "\n",
    "# Titles\n",
    "ax[0].title.set_text('Null Values')\n",
    "ax[1].title.set_text('Contrato')\n",
    "ax[2].title.set_text('Permanente')\n",
    "\n",
    "# Colocando os eixos em mesma ordem\n",
    "for value in range(0, 3):\n",
    "    ax[value].set_xlim([0, 210000])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b78779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "\n",
    "# plots\n",
    "sns.histplot(x=null, ax=ax[0], bins=100)\n",
    "sns.histplot(x=contract, ax=ax[1], bins=100)\n",
    "sns.histplot(x=permanent, ax=ax[2], bins=100)\n",
    "\n",
    "# title\n",
    "ax[0].title.set_text('Null Values Salary')\n",
    "ax[1].title.set_text('Contract Salary')\n",
    "ax[2].title.set_text('Permanent Salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd8337",
   "metadata": {},
   "source": [
    "## 5.4. Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dba01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import infos\n",
    "title_info = pd.read_csv('../data/df_infos_title.csv').drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# DataFrames Auxiliares\n",
    "title_corr = title_info[['Word', 'Corr']].sort_values(by='Corr', ascending=False)\n",
    "title_freq = title_info[['Word', 'Count']].sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Dicionario de contagem para a nuvem de palavras\n",
    "dict_freq_title = title_freq.set_index('Word').to_dict()['Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21281104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geranndo os subplots\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "# setanto titulos\n",
    "ax[0].title.set_text('Corr Negativa')\n",
    "ax[1].title.set_text('Corr Positiva')\n",
    "\n",
    "# plots\n",
    "sns.barplot(x='Corr', \n",
    "            y='Word', \n",
    "            data=title_corr.tail(10).sort_values(by='Corr', ascending=True), \n",
    "            color=color_red, ax=ax[0])\n",
    "\n",
    "sns.barplot(x='Corr', \n",
    "            y='Word', \n",
    "            data=title_corr.head(10), \n",
    "            color=color_blue, ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e046d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geranndo os subplots\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "# setanto titulos\n",
    "ax[0].title.set_text('Frequencia de Palavras')\n",
    "ax[1].title.set_text('Nuvem de Palavras')\n",
    "\n",
    "# plots\n",
    "sns.barplot(x='Count', \n",
    "            y='Word', \n",
    "            data=title_freq.head(10).sort_values(by='Count', ascending=False), \n",
    "            color=color_blue, ax=ax[0])\n",
    "\n",
    "\n",
    "# Nuvem de palavras\n",
    "# Color mask\n",
    "mask = np.array(Image.open(\"../img/blue_palette.png\"))\n",
    "\n",
    "# Instanciando a color mark\n",
    "image_colors = ImageColorGenerator(mask)\n",
    "\n",
    "# Configurando a WordCloud\n",
    "wc = WordCloud(background_color=\"white\", max_words=100,\n",
    "               max_font_size=256, mode='RGBA',\n",
    "               random_state=42, width=500, height=500)\n",
    "\n",
    "# Plot na axis\n",
    "ax[1] = wc.generate_from_frequencies(dict_freq_title)\n",
    "imshow(wc.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc86034",
   "metadata": {},
   "source": [
    "## 5.5. Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64495ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import infos\n",
    "desc_info = pd.read_csv('../data/df_infos_desc.csv').drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# DataFrames Auxiliares\n",
    "desc_corr = desc_info[['Word', 'Corr']].sort_values(by='Corr', ascending=False)\n",
    "desc_freq = desc_info[['Word', 'Count']].sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Dicionario de contagem para a nuvem de palavras\n",
    "dict_freq_desc = desc_freq.set_index('Word').to_dict()['Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfc706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geranndo os subplots\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "# setanto titulos\n",
    "ax[0].title.set_text('Corr Negativa')\n",
    "ax[1].title.set_text('Corr Positiva')\n",
    "\n",
    "# plots\n",
    "sns.barplot(x='Corr', \n",
    "            y='Word', \n",
    "            data=desc_corr.tail(10).sort_values(by='Corr', ascending=True), \n",
    "            color=color_red, ax=ax[0])\n",
    "\n",
    "sns.barplot(x='Corr', \n",
    "            y='Word', \n",
    "            data=desc_corr.head(10), \n",
    "            color=color_blue, ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee30aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geranndo os subplots\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "# setanto titulos\n",
    "ax[0].title.set_text('Frequencia de Palavras')\n",
    "ax[1].title.set_text('Nuvem de Palavras')\n",
    "\n",
    "# plots\n",
    "sns.barplot(x='Count', \n",
    "            y='Word', \n",
    "            data=desc_freq.head(10).sort_values(by='Count', ascending=False), \n",
    "            color=color_blue, ax=ax[0])\n",
    "\n",
    "\n",
    "# Nuvem de palavras\n",
    "# Color mask\n",
    "mask = np.array(Image.open(\"../img/blue_palette.png\"))\n",
    "\n",
    "# Instanciando a color mark\n",
    "image_colors = ImageColorGenerator(mask)\n",
    "\n",
    "# Configurando a WordCloud\n",
    "wc = WordCloud(background_color=\"white\", max_words=100,\n",
    "               max_font_size=256, mode='RGBA',\n",
    "               random_state=42, width=500, height=500)\n",
    "\n",
    "# Plot na axis\n",
    "ax[1] = wc.generate_from_frequencies(dict_freq_desc)\n",
    "imshow(wc.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f1393",
   "metadata": {},
   "source": [
    "# 6.0. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646eaba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_6 = X_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dd7ab0",
   "metadata": {},
   "source": [
    "## 6.1. Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bbe045",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_6 = X_train.drop(['SalaryNormalized', 'Title', 'FullDescription', 'LocationRaw', 'City'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a84dc93",
   "metadata": {},
   "source": [
    "## 6.2. NA Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34928b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_6.loc[X_train['ContractType'].isnull(), 'ContractType'] = 'full_time'\n",
    "X_train_6.loc[X_train['ContractTime'].isnull(), 'ContractTime'] = 'permanent'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c17be0e",
   "metadata": {},
   "source": [
    "## 6.3. Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea356fe",
   "metadata": {},
   "source": [
    "### 6.3.1. One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "trf = ColumnTransformer(transformers =[\n",
    "    ('enc', OneHotEncoder(sparse = False, drop ='first'), list(range(3))),\n",
    "], remainder ='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c638cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_6 = pd.DataFrame(trf.fit_transform(X_train_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a364e9",
   "metadata": {},
   "source": [
    "### 6.2.3. Join Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda1f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dummy Columns\n",
    "dummy_cols_title = pd.read_csv('../data/dummy_cols_title_3.csv', index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97c2bd1",
   "metadata": {},
   "source": [
    "# 7.0. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b04838",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = pd.concat([X_train_6, dummy_cols_title.reset_index(drop=True), dummies_city.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a31128",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t.columns = [value for value in range(1, len(X_train_t.columns) + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbad83f",
   "metadata": {},
   "source": [
    "# 8.0. Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32268276",
   "metadata": {},
   "source": [
    "## 8.1. Baseline - Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee8410",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = baseline_performance(y_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb6b2bc",
   "metadata": {},
   "source": [
    "## 8.2. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d3e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "lr_model = cross_val_performance(X_train_t, y_train, cv=5, model=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b713d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b189660",
   "metadata": {},
   "source": [
    "## 8.3. DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b56b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "dtr_model = cross_val_performance(X_train_t, y_train, cv=5, model=dtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d78bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb36100",
   "metadata": {},
   "source": [
    "## 8.4. Comparance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13329a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_performance = pd.concat([base, lr_model, dtr_model]).sort_values(by='MAE').reset_index(drop=True)\n",
    "\n",
    "cross_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save performance\n",
    "cross_performance.to_csv('../data/cross_performance_c05_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a9447",
   "metadata": {},
   "source": [
    "# 9.0. Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf096234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_evaluation(dataset, columns_title, city_api_list):\n",
    "    def __init__():\n",
    "        self.dataset = dataset\n",
    "        self.dummy_cols_title = columns_title\n",
    "        self.city_api_list = city_api_list\n",
    "    \n",
    "    def select_data(self):\n",
    "        X_test = dataset.loc[:, \n",
    "                            ['Title', 'LocationRaw', \n",
    "                            'LocationNormalized', 'ContractType', \n",
    "                            'ContractTime', 'Category']\n",
    "                           ]\n",
    "        self.dataset = X_test\n",
    "        \n",
    "    \n",
    "    def one_hot(self):\n",
    "        # selecionando as 3 colunas iniciais\n",
    "        dums = self.dataset.loc[:, ['ContractType', 'ContractTime', 'Category']]\n",
    "\n",
    "        # Filtrando null values\n",
    "        dums.loc[dums['ContractType'].isnull(), 'ContractType'] = 'full_time'\n",
    "        dums.loc[dums['ContractTime'].isnull(), 'ContractTime'] = 'permanent'\n",
    "\n",
    "        # Transformando as colunas em binarias\n",
    "        trf = ColumnTransformer(transformers =[\n",
    "            ('enc', OneHotEncoder(sparse = False, drop ='first'), list(range(3))),\n",
    "        ], remainder ='passthrough')\n",
    "\n",
    "        dums = pd.DataFrame(trf.fit_transform(dums))\n",
    "        \n",
    "        self.one_hot_ccc = dums\n",
    "        \n",
    "    \n",
    "    def concat_data(self):\n",
    "        title_corr_final = [word for word in dummy_cols_title.columns]\n",
    "        \n",
    "        X_test_t = pd.concat([self.dataset.loc[:, \n",
    "                                         ['Title', \n",
    "                                          'LocationRaw', \n",
    "                                          'LocationNormalized']].reset_index(drop=True), \n",
    "                              dums], axis=1)\n",
    "        \n",
    "        self.dataset = X_test_t\n",
    "        \n",
    "    \n",
    "    def title_transform(self):\n",
    "        # Tokenizando\n",
    "        self.dataset['Title'] = self.dataset['Title'].apply(lambda x: tokenize(x))\n",
    "\n",
    "        # Transformando os tokens\n",
    "        for value in self.dummy_cols_title:\n",
    "            self.dataset[value] = self.dataset['Title'].apply(lambda x: 1 if value in x else 0)\n",
    "    \n",
    "    \n",
    "    def city_transform(self):\n",
    "        # city\n",
    "        column_new_city = city_imputation(X_test, 'LocationRaw', \n",
    "                                          'LocationNormalized', \n",
    "                                          self.city_api_list)\n",
    "        \n",
    "        self.column_city = column_new_city\n",
    "        \n",
    "        \n",
    "    def concat_all(self):\n",
    "        self.dataset = pd.concat([self.dataset, self.column_city.reset_index(drop='True')], axis=1)\n",
    "        \n",
    "        \n",
    "    def final_preparation(self):\n",
    "        df_aux = X_test_t.loc[:, ['City', 'LocationRaw']]\n",
    "\n",
    "        for column in list_corr_city:\n",
    "            df_aux[column] = df_aux['City'].apply(lambda x: 1 if column == x else 0)\n",
    "\n",
    "        dummies_city = df_aux.drop(['City', 'LocationRaw'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b55435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e59b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ec46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_corr_final = [word for word in dummy_cols_title.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf60a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t = pd.concat([X_test.loc[:, ['Title', 'LocationRaw', 'LocationNormalized']].reset_index(drop=True), dums], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfba1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizando\n",
    "X_test_t['Title'] = X_test_t['Title'].apply(lambda x: tokenize(x))\n",
    "\n",
    "# Transformando os tokens\n",
    "for value in title_corr_final:\n",
    "    X_test_t[value] = X_test_t['Title'].apply(lambda x: 1 if value in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03347290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# city\n",
    "column_new_city = city_imputation(X_test, 'LocationRaw', 'LocationNormalized', city_api_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c11582",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t = pd.concat([X_test_t, column_new_city.reset_index(drop='True')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0562cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = X_test_t.loc[:, ['City', 'LocationRaw']]\n",
    "\n",
    "for column in list_corr_city:\n",
    "    df_aux[column] = df_aux['City'].apply(lambda x: 1 if column == x else 0)\n",
    "\n",
    "dummies_city = df_aux.drop(['City', 'LocationRaw'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t = pd.concat([X_test_t, dummies_city], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1bb4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t = X_test_t.drop(['City', 'LocationRaw', 'LocationNormalized', 'Title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t.columns = [value for value in range(1, len(X_test_t.columns) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0dbcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "dtr.fit(X_train_t, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbbd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = dtr.predict(X_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb4ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a1587",
   "metadata": {},
   "source": [
    "# 10.0. Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c342061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639fd86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
